{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n#os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:07:37.511423Z","iopub.execute_input":"2025-12-14T14:07:37.511965Z","iopub.status.idle":"2025-12-14T14:07:37.517911Z","shell.execute_reply.started":"2025-12-14T14:07:37.511942Z","shell.execute_reply":"2025-12-14T14:07:37.517100Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install -U peft transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:07:37.529893Z","iopub.execute_input":"2025-12-14T14:07:37.530132Z","iopub.status.idle":"2025-12-14T14:08:55.085971Z","shell.execute_reply.started":"2025-12-14T14:07:37.530098Z","shell.execute_reply":"2025-12-14T14:08:55.085188Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nCollecting peft\n  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, peft\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.18.0 tokenizers-0.22.1 transformers-4.57.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# COMPLETE FIXED MEDGEMMA FINE-TUNING NOTEBOOK\n# Dataset: alvinl29/medical-vision-llm-dataset-v2\n# ============================================================================\n\n# ==========================\n# Cell 1: Setup & Installs\n# ==========================\n!pip install -q transformers datasets accelerate peft trl bitsandbytes safetensors sentencepiece\n\n# Downgrade protobuf if needed\n!pip install --upgrade --force-reinstall \"protobuf==3.20.3\"\n\nimport os\nWORKDIR = \"/content/medgemma_finetune\"\nos.makedirs(WORKDIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:08:55.087388Z","iopub.execute_input":"2025-12-14T14:08:55.087625Z","iopub.status.idle":"2025-12-14T14:09:08.153549Z","shell.execute_reply.started":"2025-12-14T14:08:55.087600Z","shell.execute_reply":"2025-12-14T14:09:08.152883Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==========================\n# Cell 2: Imports\n# ==========================\nimport gc\nimport torch\nfrom pathlib import Path\nfrom typing import Any, Dict, List\nfrom PIL import Image\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\n\nprint(\"✓ Imports successful\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:09:08.154482Z","iopub.execute_input":"2025-12-14T14:09:08.154785Z","iopub.status.idle":"2025-12-14T14:09:36.745573Z","shell.execute_reply.started":"2025-12-14T14:09:08.154750Z","shell.execute_reply":"2025-12-14T14:09:36.744812Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 14:09:18.364503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765721358.553330      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765721358.607681      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Imports successful\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==========================\n# Cell 3: Load Dataset\n# ==========================\nHF_DATASET_ID = \"alvinl29/medical-vision-llm-dataset-v2\"\nprint(f\"Loading dataset: {HF_DATASET_ID}\")\n\nraw = load_dataset(HF_DATASET_ID)\nprint(\"✓ Dataset loaded\")\nprint(f\"  - Train: {len(raw['train'])} examples\")\nprint(f\"  - Validation: {len(raw['validation'])} examples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:09:36.746992Z","iopub.execute_input":"2025-12-14T14:09:36.747255Z","iopub.status.idle":"2025-12-14T14:09:49.330048Z","shell.execute_reply.started":"2025-12-14T14:09:36.747238Z","shell.execute_reply":"2025-12-14T14:09:49.329048Z"}},"outputs":[{"name":"stdout","text":"Loading dataset: alvinl29/medical-vision-llm-dataset-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03866a90e02f4fad920151b7e043b3d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002.parquet:   0%|          | 0.00/408M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c57c63873f848e7a251557902d7317a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002.parquet:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a85afb623094054be616ad7aa4be108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3574617f2d72489e9efbd56cd2380a4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b5ffd889454361baad6c1d7c923f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20afc5c5eac14b598260a35765d8eeed"}},"metadata":{}},{"name":"stdout","text":"✓ Dataset loaded\n  - Train: 3035 examples\n  - Validation: 758 examples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:10:13.066201Z","iopub.execute_input":"2025-12-14T14:10:13.066530Z","iopub.status.idle":"2025-12-14T14:10:13.104144Z","shell.execute_reply.started":"2025-12-14T14:10:13.066506Z","shell.execute_reply":"2025-12-14T14:10:13.103403Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\ntorch.set_default_dtype(torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:09:49.393436Z","iopub.execute_input":"2025-12-14T14:09:49.393968Z","iopub.status.idle":"2025-12-14T14:09:49.637759Z","shell.execute_reply.started":"2025-12-14T14:09:49.393949Z","shell.execute_reply":"2025-12-14T14:09:49.636723Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ==========================\n# Cell 4: Login to HuggingFace\n# ==========================\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:10:17.122221Z","iopub.execute_input":"2025-12-14T14:10:17.122882Z","iopub.status.idle":"2025-12-14T14:10:17.142939Z","shell.execute_reply.started":"2025-12-14T14:10:17.122854Z","shell.execute_reply":"2025-12-14T14:10:17.142062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e038b297523a4af8b788a4c23c940768"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# ==========================\\n\n# Cell 5: Load Model & Processor (FIXED FOR T4/P100)\n# ==========================\\n\nBASE_MODEL = \"google/medgemma-4b-it\"\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16, \n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nprint(f\"Loading model: {BASE_MODEL}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    dtype=torch.float16,  # <--- CRITICAL FIX: Forces all weights to FP16\n)\n\nprocessor = AutoProcessor.from_pretrained(BASE_MODEL)\nprocessor.tokenizer.padding_side = \"right\"\nif processor.tokenizer.pad_token_id is None:\n    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n\nprint(\"✓ Model and processor loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:10:33.735047Z","iopub.execute_input":"2025-12-14T14:10:33.735553Z","iopub.status.idle":"2025-12-14T14:11:37.485915Z","shell.execute_reply.started":"2025-12-14T14:10:33.735528Z","shell.execute_reply":"2025-12-14T14:11:37.485150Z"}},"outputs":[{"name":"stdout","text":"Loading model: google/medgemma-4b-it\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee475d88b064240a6ffa0e85ec0f3a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdb85ccf425482cb135135c8ce3dc23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c377947540d54be8bef2f899f26735b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19671d4ee1d34321ab7705c0f23453f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7026dc59b77445fa787952d8878f5ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0e26243e1944b18124dc1ad9fc47cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7926ee5b48604e5a9ed4342355f8170a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abe6d32da154ed6981c511064999495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ec944560074888a0f8505ca57ef5c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24de66379124e11bd8f42d859f7a0da"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba50b468114e483c886e35c2694d5be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38915b9077349368576ac25b3d4ff04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ba7fbb7edf4aa889aebefefbbcc5c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8080312ed5314880a59eae26f04d0412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2491a2b044b4b20baf074df9e9ef631"}},"metadata":{}},{"name":"stdout","text":"✓ Model and processor loaded\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n# THIS IS CRITICAL\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:11:37.529761Z","iopub.execute_input":"2025-12-14T14:11:37.529970Z","iopub.status.idle":"2025-12-14T14:11:37.543323Z","shell.execute_reply.started":"2025-12-14T14:11:37.529954Z","shell.execute_reply":"2025-12-14T14:11:37.542560Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ==========================\n# Cell 6: LoRA Configuration\n# ==========================\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:00.918586Z","iopub.execute_input":"2025-12-14T14:13:00.919415Z","iopub.status.idle":"2025-12-14T14:13:03.063549Z","shell.execute_reply.started":"2025-12-14T14:13:00.919388Z","shell.execute_reply":"2025-12-14T14:13:03.062904Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 131,153,920 || all params: 4,431,233,392 || trainable%: 2.9598\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Check at least one LoRA / linear grad dtype later\nfor n, p in model.named_parameters():\n    if p.requires_grad:\n        print(n, p.dtype)\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:07.861693Z","iopub.execute_input":"2025-12-14T14:13:07.862409Z","iopub.status.idle":"2025-12-14T14:13:07.866733Z","shell.execute_reply.started":"2025-12-14T14:13:07.862385Z","shell.execute_reply":"2025-12-14T14:13:07.865888Z"}},"outputs":[{"name":"stdout","text":"base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ==========================\n# Cell 7: Format Dataset\n# ==========================\ndef format_data_to_messages(example: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Convert dataset format to chat messages format.\"\"\"\n    \n    # Build the user prompt\n    prompt = (\n        \"You are a medical vision-language assistant.\\n\"\n        \"Analyze the following image and answer the user's question.\\n\\n\"\n        f\"Image Modality: {example.get('modality', 'Unknown')}\\n\"\n        f\"Body Part: {example.get('body_part', 'Unknown')}\\n\"\n        f\"Image Description: {example.get('image_description', '')}\\n\\n\"\n        f\"Question: {example.get('question', '')}\\n\"\n        \"Answer:\"\n    )\n    \n    # Create messages structure\n    example[\"messages\"] = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": example.get('answer', '')},\n            ],\n        },\n    ]\n    \n    return example\n\n\nprint(\"Formatting datasets...\")\ntrain_formatted = raw['train'].map(\n    format_data_to_messages,\n    remove_columns=[col for col in raw['train'].column_names if col not in ['image', 'messages']]\n)\n\nval_formatted = raw['validation'].map(\n    format_data_to_messages,\n    remove_columns=[col for col in raw['validation'].column_names if col not in ['image', 'messages']]\n)\n\nprint(f\"✓ Train formatted: {len(train_formatted)} examples\")\nprint(f\"✓ Val formatted: {len(val_formatted)} examples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:12.800651Z","iopub.execute_input":"2025-12-14T14:13:12.800961Z","iopub.status.idle":"2025-12-14T14:13:15.519254Z","shell.execute_reply.started":"2025-12-14T14:13:12.800939Z","shell.execute_reply":"2025-12-14T14:13:15.518413Z"}},"outputs":[{"name":"stdout","text":"Formatting datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"228133f0ce2440e2a188702f71b36d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0dd994aac341588cb1e0831e66afd0"}},"metadata":{}},{"name":"stdout","text":"✓ Train formatted: 3035 examples\n✓ Val formatted: 758 examples\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==========================\n# Cell 8: Collate Function\n# ==========================\ndef collate_fn(examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n    \"\"\"Collate function for batching.\"\"\"\n    texts = []\n    images = []\n    \n    for example in examples:\n        # Ensure image is RGB\n        img = example[\"image\"]\n        if not isinstance(img, Image.Image):\n            img = Image.open(img)\n        img = img.convert(\"RGB\")\n        images.append([img])  # Wrap in list!\n        \n        # Apply chat template\n        text = processor.apply_chat_template(\n            example[\"messages\"],\n            add_generation_prompt=False,\n            tokenize=False,\n        ).strip()\n        texts.append(text)\n    \n    # Process batch\n    batch = processor(\n        text=texts,\n        images=images,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=2048,\n    )\n    \n    # Create labels\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    \n    # Mask prompt, keep assistant response\n    for i, example in enumerate(examples):\n        assistant_text = example[\"messages\"][1][\"content\"][0][\"text\"]\n        # Simple masking approach\n        labels[i, :batch[\"input_ids\"].shape[1] - len(assistant_text)] = -100\n    \n    batch[\"labels\"] = labels\n    return batch\n\nprint(\"✓ Collate function defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:18.903635Z","iopub.execute_input":"2025-12-14T14:13:18.903952Z","iopub.status.idle":"2025-12-14T14:13:18.911458Z","shell.execute_reply.started":"2025-12-14T14:13:18.903930Z","shell.execute_reply":"2025-12-14T14:13:18.910534Z"}},"outputs":[{"name":"stdout","text":"✓ Collate function defined\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ==========================\n# Cell 9: Training Arguments (SFTConfig)\n# ==========================\nfrom trl import SFTConfig\n\ntraining_args = SFTConfig(\n    output_dir=WORKDIR,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    fp16=False,\n    bf16=False,\n    num_train_epochs=1,  # Start with 1 epoch\n    logging_steps=50,\n    save_steps=500,\n    save_total_limit=5,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    learning_rate=1e-5,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    report_to=\"none\",\n    dataloader_num_workers=0,\n    dataloader_pin_memory=True,\n    label_names=[\"labels\"],\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    optim=\"adamw_torch_fused\",\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    max_grad_norm=None,\n)\n\nprint(\"✓ Training arguments configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:22.721276Z","iopub.execute_input":"2025-12-14T14:13:22.721787Z","iopub.status.idle":"2025-12-14T14:13:22.755601Z","shell.execute_reply.started":"2025-12-14T14:13:22.721764Z","shell.execute_reply":"2025-12-14T14:13:22.755037Z"}},"outputs":[{"name":"stdout","text":"✓ Training arguments configured\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ==========================\n# Cell 10: Create Trainer\n# ==========================\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_formatted,\n    eval_dataset=val_formatted,\n    processing_class=processor,\n    data_collator=collate_fn,\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✅ READY TO TRAIN!\")\nprint(\"=\"*60)\nprint(f\"\\nDataset: {len(train_formatted)} train, {len(val_formatted)} val\")\nprint(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"Total steps: ~{len(train_formatted) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:26.983196Z","iopub.execute_input":"2025-12-14T14:13:26.983483Z","iopub.status.idle":"2025-12-14T14:13:27.079995Z","shell.execute_reply.started":"2025-12-14T14:13:26.983461Z","shell.execute_reply":"2025-12-14T14:13:27.079401Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n✅ READY TO TRAIN!\n============================================================\n\nDataset: 3035 train, 758 val\nEffective batch size: 8\nTotal steps: ~379\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(\"Model dtype:\", next(model.parameters()).dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:31.020967Z","iopub.execute_input":"2025-12-14T14:13:31.021655Z","iopub.status.idle":"2025-12-14T14:13:31.025541Z","shell.execute_reply.started":"2025-12-14T14:13:31.021631Z","shell.execute_reply":"2025-12-14T14:13:31.024889Z"}},"outputs":[{"name":"stdout","text":"Model dtype: torch.float32\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(trainer.args.fp16, trainer.args.bf16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:33.289957Z","iopub.execute_input":"2025-12-14T14:13:33.290719Z","iopub.status.idle":"2025-12-14T14:13:33.294812Z","shell.execute_reply.started":"2025-12-14T14:13:33.290694Z","shell.execute_reply":"2025-12-14T14:13:33.294037Z"}},"outputs":[{"name":"stdout","text":"False False\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"fp16:\", trainer.args.fp16)\nprint(\"bf16:\", trainer.args.bf16)\nprint(\"use_amp:\", trainer.accelerator.use_fp16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:39.166396Z","iopub.execute_input":"2025-12-14T14:13:39.166680Z","iopub.status.idle":"2025-12-14T14:13:39.204874Z","shell.execute_reply.started":"2025-12-14T14:13:39.166658Z","shell.execute_reply":"2025-12-14T14:13:39.203948Z"}},"outputs":[{"name":"stdout","text":"fp16: False\nbf16: False\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3307987912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fp16:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bf16:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbf16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_amp:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_fp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'Accelerator' object has no attribute 'use_fp16'"],"ename":"AttributeError","evalue":"'Accelerator' object has no attribute 'use_fp16'","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"print(next(model.parameters()).dtype)  # float16\nprint(trainer.args.fp16, trainer.args.bf16)  # True False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:45.781061Z","iopub.execute_input":"2025-12-14T14:13:45.781676Z","iopub.status.idle":"2025-12-14T14:13:45.786063Z","shell.execute_reply.started":"2025-12-14T14:13:45.781649Z","shell.execute_reply":"2025-12-14T14:13:45.785187Z"}},"outputs":[{"name":"stdout","text":"torch.float32\nFalse False\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\n\nbf16_params = []\nfor name, p in model.named_parameters():\n    if p.requires_grad and p.dtype == torch.bfloat16:\n        bf16_params.append(name)\n\nprint(\"BF16 trainable params:\", bf16_params[:10])\nprint(\"Count:\", len(bf16_params))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:51.928060Z","iopub.execute_input":"2025-12-14T14:13:51.928340Z","iopub.status.idle":"2025-12-14T14:13:51.946665Z","shell.execute_reply.started":"2025-12-14T14:13:51.928319Z","shell.execute_reply":"2025-12-14T14:13:51.945963Z"}},"outputs":[{"name":"stdout","text":"BF16 trainable params: ['base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight']\nCount: 638\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"bf16_params = [\n    n for n, p in model.named_parameters()\n    if p.requires_grad and p.dtype == torch.bfloat16\n]\nprint(len(bf16_params))  # must be 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:56.038228Z","iopub.execute_input":"2025-12-14T14:13:56.038992Z","iopub.status.idle":"2025-12-14T14:13:56.059425Z","shell.execute_reply.started":"2025-12-14T14:13:56.038958Z","shell.execute_reply":"2025-12-14T14:13:56.058678Z"}},"outputs":[{"name":"stdout","text":"638\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom collections import Counter\n\ndef audit_dtypes(model, trainer=None):\n    print(\"\\n===== GLOBAL SETTINGS =====\")\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"Default dtype:\", torch.get_default_dtype())\n    print(\"AMP autocast enabled:\", torch.is_autocast_enabled())\n\n    if trainer is not None:\n        print(\"\\n===== TRAINER / ACCELERATE =====\")\n        print(\"fp16:\", trainer.args.fp16)\n        print(\"bf16:\", trainer.args.bf16)\n        try:\n            print(\"accelerate mixed_precision:\", trainer.accelerator.state.mixed_precision)\n        except Exception:\n            print(\"accelerate mixed_precision: <unknown>\")\n\n    print(\"\\n===== PARAMETER DTYPES (SUMMARY) =====\")\n    param_dtypes = Counter(p.dtype for p in model.parameters())\n    for k, v in param_dtypes.items():\n        print(f\"{k}: {v}\")\n\n    print(\"\\n===== TRAINABLE PARAMETER DTYPES =====\")\n    trainable = Counter(p.dtype for p in model.parameters() if p.requires_grad)\n    for k, v in trainable.items():\n        print(f\"{k}: {v}\")\n\n    print(\"\\n===== LoRA PARAMETER DTYPES =====\")\n    lora_dtypes = Counter()\n    for name, p in model.named_parameters():\n        if \"lora\" in name.lower():\n            lora_dtypes[p.dtype] += 1\n    if lora_dtypes:\n        for k, v in lora_dtypes.items():\n            print(f\"{k}: {v}\")\n    else:\n        print(\"No LoRA parameters found\")\n\n    print(\"\\n===== BF16 TRAINABLE PARAMETERS (NAMES) =====\")\n    bf16_names = [\n        name for name, p in model.named_parameters()\n        if p.requires_grad and p.dtype == torch.bfloat16\n    ]\n    if bf16_names:\n        print(\"Count:\", len(bf16_names))\n        print(\"First 10:\")\n        for n in bf16_names[:10]:\n            print(\" \", n)\n    else:\n        print(\"None\")\n\n    print(\"\\n===== FP16 TRAINABLE PARAMETERS (NAMES) =====\")\n    fp16_names = [\n        name for name, p in model.named_parameters()\n        if p.requires_grad and p.dtype == torch.float16\n    ]\n    if fp16_names:\n        print(\"Count:\", len(fp16_names))\n        print(\"First 10:\")\n        for n in fp16_names[:10]:\n            print(\" \", n)\n    else:\n        print(\"None\")\n\n    print(\"\\n===== FP32 TRAINABLE PARAMETERS (NAMES) =====\")\n    fp32_names = [\n        name for name, p in model.named_parameters()\n        if p.requires_grad and p.dtype == torch.float32\n    ]\n    if fp32_names:\n        print(\"Count:\", len(fp32_names))\n        print(\"First 10:\")\n        for n in fp32_names[:10]:\n            print(\" \", n)\n    else:\n        print(\"None\")\n\n    print(\"\\n===== DONE =====\\n\")\n\n\n# USAGE:\naudit_dtypes(model, trainer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:13:59.429071Z","iopub.execute_input":"2025-12-14T14:13:59.429351Z","iopub.status.idle":"2025-12-14T14:13:59.498146Z","shell.execute_reply.started":"2025-12-14T14:13:59.429329Z","shell.execute_reply":"2025-12-14T14:13:59.497364Z"}},"outputs":[{"name":"stdout","text":"\n===== GLOBAL SETTINGS =====\nCUDA available: True\nDefault dtype: torch.float16\nAMP autocast enabled: False\n\n===== TRAINER / ACCELERATE =====\nfp16: False\nbf16: False\naccelerate mixed_precision: no\n\n===== PARAMETER DTYPES (SUMMARY) =====\ntorch.float32: 483\ntorch.uint8: 400\ntorch.bfloat16: 638\n\n===== TRAINABLE PARAMETER DTYPES =====\ntorch.bfloat16: 638\n\n===== LoRA PARAMETER DTYPES =====\ntorch.bfloat16: 638\n\n===== BF16 TRAINABLE PARAMETERS (NAMES) =====\nCount: 638\nFirst 10:\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n  base_model.model.base_model.model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n\n===== FP16 TRAINABLE PARAMETERS (NAMES) =====\nNone\n\n===== FP32 TRAINABLE PARAMETERS (NAMES) =====\nNone\n\n===== DONE =====\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ==========================\n# Cell 11: Train!\n# ==========================\nprint(\"\\n🚀 Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:14:05.549658Z","iopub.execute_input":"2025-12-14T14:14:05.549969Z","iopub.status.idle":"2025-12-14T22:29:29.896424Z","shell.execute_reply.started":"2025-12-14T14:14:05.549946Z","shell.execute_reply":"2025-12-14T22:29:29.895766Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [380/380 8:14:07, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=380, training_loss=1.3419783140483657, metrics={'train_runtime': 29723.6789, 'train_samples_per_second': 0.102, 'train_steps_per_second': 0.013, 'total_flos': 2.462890294676928e+16, 'train_loss': 1.3419783140483657, 'entropy': 3.486027053061952, 'num_tokens': 1093084.0, 'mean_token_accuracy': 0.842724609375, 'epoch': 1.0})"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# ==========================\n# Cell 12: Save Adapter\n# ==========================\nWORKDIR1=\"/kaggle/working/\"\nADAPTER_DIR = os.path.join(WORKDIR1, \"final_adapter\")\nos.makedirs(ADAPTER_DIR, exist_ok=True)\n\nprint(f\"\\n💾 Saving adapter to {ADAPTER_DIR}...\")\nmodel.save_pretrained(ADAPTER_DIR)\nprocessor.save_pretrained(ADAPTER_DIR)\n\nprint(\"✅ Training complete!\")\nprint(f\"📁 Adapter saved to: {ADAPTER_DIR}\")\n\n# Optional: Create zip for download\nimport shutil\nzip_path = ADAPTER_DIR + \".zip\"\nshutil.make_archive(ADAPTER_DIR, 'zip', ADAPTER_DIR)\nprint(f\"📦 Zip created: {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:32:48.425225Z","iopub.execute_input":"2025-12-14T22:32:48.425547Z","iopub.status.idle":"2025-12-14T22:33:22.780005Z","shell.execute_reply.started":"2025-12-14T22:32:48.425525Z","shell.execute_reply":"2025-12-14T22:33:22.779182Z"}},"outputs":[{"name":"stdout","text":"\n💾 Saving adapter to /kaggle/working/final_adapter...\n✅ Training complete!\n📁 Adapter saved to: /kaggle/working/final_adapter\n📦 Zip created: /kaggle/working/final_adapter.zip\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!ls /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:36:13.672747Z","iopub.execute_input":"2025-12-14T22:36:13.673704Z","iopub.status.idle":"2025-12-14T22:36:13.954858Z","shell.execute_reply.started":"2025-12-14T22:36:13.673658Z","shell.execute_reply":"2025-12-14T22:36:13.954089Z"}},"outputs":[{"name":"stdout","text":"final_adapter  final_adapter.zip\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!zip -r /kaggle/working/lora_adapter.zip /kaggle/working/lora_adapter\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:38:04.393390Z","iopub.execute_input":"2025-12-14T22:38:04.393731Z","iopub.status.idle":"2025-12-14T22:38:04.656263Z","shell.execute_reply.started":"2025-12-14T22:38:04.393701Z","shell.execute_reply":"2025-12-14T22:38:04.655100Z"}},"outputs":[{"name":"stdout","text":"\tzip warning: name not matched: /kaggle/working/lora_adapter\n\nzip error: Nothing to do! (try: zip -r /kaggle/working/lora_adapter.zip . -i /kaggle/working/lora_adapter)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"!pip install -q PyDrive\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:40:58.169564Z","iopub.execute_input":"2025-12-14T22:40:58.170197Z","iopub.status.idle":"2025-12-14T22:41:02.205204Z","shell.execute_reply.started":"2025-12-14T22:40:58.170162Z","shell.execute_reply":"2025-12-14T22:41:02.204329Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":32}]}