{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Run after kernel restart\nimport gc, os, shutil, torch\ngc.collect()\ntorch.cuda.empty_cache()\n# Optional: remove datasets cache (careful: this deletes dataset cache files)\ncache_dir = os.path.expanduser('~/.cache/huggingface/datasets')\nif os.path.exists(cache_dir):\n    print(\"Clearing HF datasets cache (may free large disk but will re-download if needed)...\")\n    # shutil.rmtree(cache_dir)  # uncomment only if you want to clear cache\nprint(\"Cleanup done. Re-run next steps.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:28:24.422441Z","iopub.execute_input":"2025-12-12T16:28:24.423188Z","iopub.status.idle":"2025-12-12T16:28:27.928433Z","shell.execute_reply.started":"2025-12-12T16:28:24.423163Z","shell.execute_reply":"2025-12-12T16:28:27.927632Z"}},"outputs":[{"name":"stdout","text":"Cleanup done. Re-run next steps.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# MedGemma Fine-tune + Eval + Adapter Save (2xT4)\n# Notebook-style Python script with cell separators and explanatory comments.\n# Designed for: alvinl29/medical-vision-llm-dataset-v2 (HF parquet; columns: image, question, answer, image_description, body_part, modality, conversations)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:28:27.929432Z","iopub.execute_input":"2025-12-12T16:28:27.929770Z","iopub.status.idle":"2025-12-12T16:28:27.933189Z","shell.execute_reply.started":"2025-12-12T16:28:27.929752Z","shell.execute_reply":"2025-12-12T16:28:27.932330Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==========================\n# Cell 0: Setup environment\n# - Installs required libraries (run once)\n# - Mounts Drive if needed\n# - Sets useful environment variables\n# ==========================\n\n\n #pip installs (uncomment to run in a fresh Colab/Kaggle env)\n!pip install -q transformers datasets accelerate peft bitsandbytes safetensors sentencepiece transformers[torch] torchvision timm git+https://github.com/huggingface/peft.git\n# Note: On Kaggle, some packages may be preinstalled; adjust as needed.\n\n\n# If you use Google Drive for checkpoints, mount it (Colab only)\n# from google.colab import drive\n# drive.mount('/content/gdrive')\n\n# Working directory for checkpoints and artifacts\nWORKDIR = \"/content/medgemma_finetune\" # change to your Drive path if needed\nimport os\nos.makedirs(WORKDIR, exist_ok=True)\nprint(\"Workdir:\", WORKDIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:28:27.934342Z","iopub.execute_input":"2025-12-12T16:28:27.934578Z","iopub.status.idle":"2025-12-12T16:29:48.318555Z","shell.execute_reply.started":"2025-12-12T16:28:27.934563Z","shell.execute_reply":"2025-12-12T16:29:48.317577Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mWorkdir: /content/medgemma_finetune\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# install protobuf that restores the legacy API\n!pip install --upgrade --force-reinstall \"protobuf==3.20.3\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:29:48.320759Z","iopub.execute_input":"2025-12-12T16:29:48.321102Z","iopub.status.idle":"2025-12-12T16:29:52.424696Z","shell.execute_reply.started":"2025-12-12T16:29:48.321077Z","shell.execute_reply":"2025-12-12T16:29:52.423984Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==========================\n# Cell 1: Imports & utility functions\n# - Central imports, monitoring helpers\n# ==========================\n\n\nimport gc\nimport time\nimport json\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\nimport torch\nimport numpy as np\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\n# optional: psutil to monitor memory\ntry:\n    import psutil\nexcept Exception:\n    psutil = None\n\n\ndef print_mem(prefix: str = \"\"):\n    \"\"\"Print simple CPU + GPU memory stats to help monitoring.\"\"\"\n    if psutil:\n        vm = psutil.virtual_memory()\n        print(f\"{prefix} CPU RAM: {vm.available/1024**3:.2f} GB available / {vm.total/1024**3:.2f} GB total\")\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)} - {torch.cuda.memory_reserved(i)/1024**3:.2f} GB reserved, {torch.cuda.memory_allocated(i)/1024**3:.2f} GB allocated\")\n\nprint_mem(\"startup\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:29:52.425703Z","iopub.execute_input":"2025-12-12T16:29:52.425965Z","iopub.status.idle":"2025-12-12T16:30:23.300685Z","shell.execute_reply.started":"2025-12-12T16:29:52.425942Z","shell.execute_reply":"2025-12-12T16:30:23.299764Z"}},"outputs":[{"name":"stderr","text":"2025-12-12 16:30:02.580550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765557002.771060      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765557002.826593      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"startup CPU RAM: 29.26 GB available / 31.35 GB total\nGPU 0: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\nGPU 1: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================\n# Cell 2: Dataset inspection (HF parquet)\n# - Loads the HF dataset ID and prints schema + sample rows\n# - USER: confirm dataset_id below\n# ==========================\n\nHF_DATASET_ID = \"alvinl29/medical-vision-llm-dataset-v2\"\nprint(\"Loading dataset id:\", HF_DATASET_ID)\nraw = load_dataset(HF_DATASET_ID)\nprint(\"Splits:\", list(raw.keys()))\nfor s in raw:\n    print(s, len(raw[s]))\n\nprint(\"Train columns:\", raw['train'].column_names)\nprint(\"Sample train row (first):\")\nprint(raw['train'][0])\n\n# Inspect the 'image' feature type and one image if present\nif 'image' in raw['train'].column_names:\n    print(\"Image feature type:\", raw['train'].features['image'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:30:23.301591Z","iopub.execute_input":"2025-12-12T16:30:23.302543Z","iopub.status.idle":"2025-12-12T16:30:36.807250Z","shell.execute_reply.started":"2025-12-12T16:30:23.302519Z","shell.execute_reply":"2025-12-12T16:30:36.806585Z"}},"outputs":[{"name":"stdout","text":"Loading dataset id: alvinl29/medical-vision-llm-dataset-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04aa5b0695da45c199b90ed0934cd9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002.parquet:   0%|          | 0.00/408M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583118281cfb40cba61711542bcb0ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002.parquet:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b0550f577c4a01a4e4914e0ed2c6c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6f348bd28948a8884b85cb34757924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065512c61413450f8567b9c7397fd19e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cbf5b8d7d6241d881ee1a05531fb626"}},"metadata":{}},{"name":"stdout","text":"Splits: ['train', 'validation']\ntrain 3035\nvalidation 758\nTrain columns: ['image', 'conversations', 'image_description', 'question', 'answer', 'dataset_source', 'modality', 'body_part', 'sample_id', 'instruction']\nSample train row (first):\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=800x969 at 0x7CEA902E3DD0>, 'conversations': {'role': ['user', 'assistant'], 'content': ['Describe the medical findings in this image.', 'Plain abdominal radiographs suggested subileus.']}, 'image_description': 'Plain abdominal radiographs suggested subileus.', 'question': 'Describe the medical findings.', 'answer': 'Plain abdominal radiographs suggested subileus.', 'dataset_source': 'ROCO', 'modality': 'X-ray', 'body_part': 'Abdomen', 'sample_id': 'roco_405', 'instruction': 'Analyze this medical image.'}\nImage feature type: Image(mode=None, decode=True)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==========================\n# Cell 3: Column mapping & prompt template\n# - Set the exact column names observed in your dataset\n# - Define the prompt template used for training\n# ==========================\n\n\n# Column mapping (edit if your dataset uses different names)\nIMAGE_COL = 'image'\nQUESTION_COL = 'question'\nANSWER_COL = 'answer'\nIMAGE_DESC_COL = 'image_description' # note: fix exact spelling in dataset if different\nBODY_PART_COL = 'body_part'\nMODALITY_COL = 'modality'\n# conversations and instruction exist but we'll use question + image_description as canonical\n\n\n# Prompt template: keeps metadata and question; model must generate the answer after 'Answer:'\nPROMPT_TEMPLATE = (\n\"You are a medical vision-language assistant.\\n\"\n\"Analyze the following image and answer the user's question.\\n\\n\"\n\"Image Modality: {modality}\\n\"\n\"Body Part: {body_part}\\n\"\n\"Image Description: {image_description}\\n\\n\"\n\"Question: {question}\\n\"\n\"Answer:\"\n)\n\n\nprint(\"Prompt example:\\n\", PROMPT_TEMPLATE.format(\nmodality=\"X-ray\", body_part=\"Abdomen\", image_description=\"Plain abdominal radiographs suggested subileus.\", question=\"Describe the medical findings.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:30:40.117996Z","iopub.execute_input":"2025-12-12T16:30:40.118411Z","iopub.status.idle":"2025-12-12T16:30:40.124377Z","shell.execute_reply.started":"2025-12-12T16:30:40.118369Z","shell.execute_reply":"2025-12-12T16:30:40.123459Z"}},"outputs":[{"name":"stdout","text":"Prompt example:\n You are a medical vision-language assistant.\nAnalyze the following image and answer the user's question.\n\nImage Modality: X-ray\nBody Part: Abdomen\nImage Description: Plain abdominal radiographs suggested subileus.\n\nQuestion: Describe the medical findings.\nAnswer:\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:30:43.982808Z","iopub.execute_input":"2025-12-12T16:30:43.983097Z","iopub.status.idle":"2025-12-12T16:30:44.003830Z","shell.execute_reply.started":"2025-12-12T16:30:43.983074Z","shell.execute_reply":"2025-12-12T16:30:44.002955Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cb38091cd841689fe88c04373d924a"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# ==========================\n# Cell 4: Tokenizer + Processor + model config placeholders\n# - Choose base model name (change to the exact HF medgemma variant you want)\n# - Prepare BitsAndBytesConfig for 4-bit loading\n# ==========================\n\nBASE_MODEL = \"google/medgemma-4b-it\"  # change if you have a different repo id\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n# Attempt to load a multimodal processor if available; otherwise we'll use PIL transforms\ntry:\n    processor = AutoProcessor.from_pretrained(BASE_MODEL)\n    print(\"Loaded AutoProcessor for model\")\nexcept Exception as e:\n    processor = None\n    print(\"No AutoProcessor available; will apply manual PIL transforms if needed.\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:31:02.461168Z","iopub.execute_input":"2025-12-12T16:31:02.461499Z","iopub.status.idle":"2025-12-12T16:31:10.257951Z","shell.execute_reply.started":"2025-12-12T16:31:02.461475Z","shell.execute_reply":"2025-12-12T16:31:10.257245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe348a19334949dab3c5f4c8a70c3c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c24767b458f4931a420b4d3fdf353a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b621b5a10d46a3bc097e2802e2f195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8025eb20ed8448a1954d5533726a23c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9bc48e4873345278c16247aae67bfb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f87e99f61c04d97b05e780eaaedb13a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42fc28e9aa4f44d58d9bb124b88c6b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e27b9c6aca4c0b915cd00fb231b5f8"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Loaded AutoProcessor for model\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==========================\n# Cell 5: Preprocessing function (CORRECTED)\n# - ONLY builds prompt strings.\n# - Does NOT tokenize. We leave that for the processor in the collator.\n# ==========================\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\ndef build_prompt_str(row):\n    \"\"\"\n    Build the textual prompt. Must include the <image> token.\n    \"\"\"\n    modality = row.get(MODALITY_COL, \"\") or \"Unknown\"\n    body_part = row.get(BODY_PART_COL, \"\") or \"Unknown\"\n    image_desc = row.get(IMAGE_DESC_COL, \"\") or \"\"\n    question = row.get(QUESTION_COL, \"\") or \"\"\n\n    # Ensure we use the exact tag the processor expects\n    prompt = (\n        \"<image>\\n\"\n        \"You are a medical vision-language assistant.\\n\"\n        \"Analyze the following image and answer the user's question.\\n\\n\"\n        f\"Image Modality: {modality}\\n\"\n        f\"Body Part: {body_part}\\n\"\n        f\"Image Description: {image_desc}\\n\\n\"\n        f\"Question: {question}\\n\"\n        \"Answer:\"\n    )\n    return prompt\n\ndef preprocess_batch_light(batch):\n    \"\"\"\n    Revised preprocessing:\n    - Just adds the 'text_prompt' and 'text_answer' fields.\n    - Does NOT tokenize inputs or remove images.\n    \"\"\"\n    prompts = []\n    answers = []\n    n = len(batch[QUESTION_COL])\n    \n    for i in range(n):\n        row = {k: batch[k][i] for k in batch.keys()}\n        prompts.append(build_prompt_str(row))\n        answers.append(row.get(ANSWER_COL, \"\") or \"\")\n\n    return {\n        \"text_prompt\": prompts,\n        \"text_answer\": answers,\n        # We keep the original image column automatically\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:31:15.214734Z","iopub.execute_input":"2025-12-12T16:31:15.215018Z","iopub.status.idle":"2025-12-12T16:31:15.221776Z","shell.execute_reply.started":"2025-12-12T16:31:15.214994Z","shell.execute_reply":"2025-12-12T16:31:15.221104Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ==========================\n# Cell 6: Prepare datasets (map preprocess)\n# - Applies preprocessing to train and validation splits\n# - Optionally create small samples for quick debug\n# ==========================\n\nprint_mem('before dataset map')\n\ntrain_ds = raw['train']\nval_ds = raw['validation'] if 'validation' in raw else raw['test'] if 'test' in raw else None\n\n# For debug: create small subsets\n# tiny_train = train_ds.select(range(min(32, len(train_ds))))\n# tiny_val = val_ds.select(range(min(16, len(val_ds))))\n\nprint(\"Mapping train dataset... this may take time\")\ntrain_proc = train_ds.map(preprocess_batch_light, batched=True, batch_size=64, remove_columns=[col for col in train_ds.column_names if col!='image'])\n\nif val_ds is not None:\n    val_proc = val_ds.map(preprocess_batch_light, batched=True, remove_columns=val_ds.column_names)\nelse:\n    val_proc = None\n\nprint(\"Mapped datasets. Train examples:\", len(train_proc), \"Val examples:\", len(val_proc) if val_proc else 0)\nprint_mem('after dataset map')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:31:19.052480Z","iopub.execute_input":"2025-12-12T16:31:19.052776Z","iopub.status.idle":"2025-12-12T16:41:42.933708Z","shell.execute_reply.started":"2025-12-12T16:31:19.052754Z","shell.execute_reply":"2025-12-12T16:41:42.933022Z"}},"outputs":[{"name":"stdout","text":"before dataset map CPU RAM: 28.08 GB available / 31.35 GB total\nGPU 0: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\nGPU 1: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\nMapping train dataset... this may take time\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7039ee498a445288272f833e6faa189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/758 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76001be682664b48b09b10f5691e6c32"}},"metadata":{}},{"name":"stdout","text":"Mapped datasets. Train examples: 3035 Val examples: 758\nafter dataset map CPU RAM: 28.15 GB available / 31.35 GB total\nGPU 0: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\nGPU 1: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==========================\n# Cell 7: Collator (CORRECTED)\n# - Handles tokenization + Image processing together\n# - Dynamically creates labels by masking the user prompt\n# ==========================\nimport torch\n\nclass OnTheFlyCollator:\n    def __init__(self, processor):\n        self.processor = processor\n        # Fallback padding token if processor doesn't set it\n        if self.processor.tokenizer.pad_token_id is None:\n             self.processor.tokenizer.pad_token_id = self.processor.tokenizer.eos_token_id\n\n    def __call__(self, features):\n        images = [f[IMAGE_COL] for f in features]\n        prompts = [f['text_prompt'] for f in features]\n        answers = [f['text_answer'] for f in features]\n        \n        # 1. Prepare full input (Prompt + Answer)\n        # The processor handles <image> expansion automatically here\n        full_texts = [p + \" \" + a for p, a in zip(prompts, answers)]\n        \n        batch = self.processor(\n            text=full_texts,\n            images=images,\n            padding=True,\n            truncation=True,\n            max_length=256, # Adjust based on your memory constraints\n            return_tensors=\"pt\"\n        )\n        \n        # 2. Create Labels (Masking the instruction/prompt part)\n        input_ids = batch['input_ids']\n        labels = input_ids.clone()\n        \n        # We need to calculate where the \"Answer\" starts to mask everything before it\n        # We do this by processing JUST the prompt to get its length\n        # Note: This adds a tiny overhead but is the safest way to get exact token alignment\n        prompt_batch = self.processor(\n            text=prompts,\n            images=images,\n            padding=\"longest\", # Padding strategy might differ, but we only need lengths\n            truncation=True,\n            max_length=256,\n            return_tensors=\"pt\"\n        )\n        \n        # Mask out the prompt tokens in the labels\n        for i, prompt_ids in enumerate(prompt_batch['input_ids']):\n            # The length of the prompt tokens\n            prompt_len = len(prompt_ids)\n            \n            # Safety check: ensure prompt length is less than total length\n            if prompt_len < len(labels[i]):\n                labels[i, :prompt_len] = -100\n            else:\n                # If prompt was truncated to be the whole sequence, ignore this sample\n                labels[i, :] = -100\n\n        batch['labels'] = labels\n        \n        return batch\n\n# Initialize the new collator\ncollator = OnTheFlyCollator(processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:44:10.028271Z","iopub.execute_input":"2025-12-12T16:44:10.028558Z","iopub.status.idle":"2025-12-12T16:44:10.036264Z","shell.execute_reply.started":"2025-12-12T16:44:10.028537Z","shell.execute_reply":"2025-12-12T16:44:10.035459Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ==========================\n# Cell 8: Model load (4-bit) + attach LoRA (for 2xT4 training)\n# - This cell loads base model in 4-bit via bitsandbytes and attaches LoRA\n# - Use accelerate/transformers device_map to automatically shard across GPUs\n# ==========================\n\nprint_mem('before model load')\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=False,\n)\n\n# LoRA config (r=16 => approx 80-120M trainable params depending on target modules)\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],  # adjust if model layer names differ\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.gradient_checkpointing_enable()\n\nprint(\"Model + LoRA prepared. Trainable params (approx):\")\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable params: {trainable:,}\")\nprint_mem('after model load')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:44:13.857488Z","iopub.execute_input":"2025-12-12T16:44:13.857985Z","iopub.status.idle":"2025-12-12T16:45:24.948807Z","shell.execute_reply.started":"2025-12-12T16:44:13.857962Z","shell.execute_reply":"2025-12-12T16:45:24.948063Z"}},"outputs":[{"name":"stdout","text":"before model load CPU RAM: 28.15 GB available / 31.35 GB total\nGPU 0: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\nGPU 1: Tesla T4 - 0.00 GB reserved, 0.00 GB allocated\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38c7a8788fe44dc92b54159b7c02dae"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835c872bfce04b68901a94a0b08ccac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8738acaa7bb498f8b211b51db7a6b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f77977fea64a688f7bd816dbc7b35c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88bc7f659a904157aaf98ef4ab612bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2229980d2ae142ae8e9957e5413ce428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384d1594473649ae919b1f8de7c0a761"}},"metadata":{}},{"name":"stdout","text":"Model + LoRA prepared. Trainable params (approx):\nTrainable params: 6,447,104\nafter model load CPU RAM: 26.09 GB available / 31.35 GB total\nGPU 0: Tesla T4 - 0.30 GB reserved, 0.14 GB allocated\nGPU 1: Tesla T4 - 4.48 GB reserved, 2.89 GB allocated\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ==========================\n# Cell 9: Custom Trainer with generation-based eval\n# - Trainer subclass overrides evaluate() to generate answers and compute metrics\n# - Metrics: exact-match and token-level F1\n# ==========================\n# ==========================\n# Cell: Metrics + Custom Trainer (no load_metric)\n# ==========================\n\nimport re\nimport torch\nfrom transformers import Trainer\n\n# ----- Normalization -----\ndef normalize_text(s):\n    s = s.lower().strip()\n    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n    s = \" \".join(s.split())\n    return s\n\n# ----- Token-level F1 -----\ndef token_f1(pred: str, gold: str):\n    p_tokens = pred.split()\n    g_tokens = gold.split()\n    if not p_tokens or not g_tokens:\n        return 0.0\n    common = set(p_tokens) & set(g_tokens)\n    prec = len(common) / len(p_tokens)\n    rec = len(common) / len(g_tokens)\n    if prec + rec == 0:\n        return 0.0\n    return 2 * (prec * rec) / (prec + rec)\n\n# ----- Custom Trainer -----\nclass MMTrainer(Trainer):\n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        ds = eval_dataset if eval_dataset is not None else self.eval_dataset\n        if ds is None:\n            return {}\n        \n        self.model.eval()\n        device = self.model.device\n        total = 0\n        matches = 0\n        f1s = 0.0\n        \n        for i in range(len(ds)):\n            ex = ds[i]\n            batch = collator([ex])\n\n            # Move required tensors\n            inputs = {\n                k: v.to(device)\n                for k, v in batch.items()\n                if k in ['input_ids', 'attention_mask', 'pixel_values']\n            }\n\n            with torch.no_grad():\n                gen = self.model.generate(\n                    **inputs,\n                    max_new_tokens=32,\n                    do_sample=False\n                )\n                output = tokenizer.decode(gen[0], skip_special_tokens=True)\n\n            # Extract prediction\n            if \"Answer:\" in output:\n                pred = output.split(\"Answer:\")[-1].strip()\n            else:\n                pred = output.strip()\n\n            gold = ex.get(\"answer\", \"\")\n            pred_n = normalize_text(pred)\n            gold_n = normalize_text(gold)\n\n            if pred_n == gold_n and gold_n != \"\":\n                matches += 1\n            f1s += token_f1(pred_n, gold_n)\n            total += 1\n        \n        accuracy = matches / total if total > 0 else 0.0\n        avg_f1 = f1s / total if total > 0 else 0.0\n        \n        metrics = {\n            f\"{metric_key_prefix}_exact_match\": accuracy,\n            f\"{metric_key_prefix}_token_f1\": avg_f1,\n            f\"{metric_key_prefix}_samples\": total\n        }\n\n        print(\"Eval metrics:\", metrics)\n        return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:46:22.612125Z","iopub.execute_input":"2025-12-12T16:46:22.612872Z","iopub.status.idle":"2025-12-12T16:46:22.622791Z","shell.execute_reply.started":"2025-12-12T16:46:22.612837Z","shell.execute_reply":"2025-12-12T16:46:22.622127Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ==========================\n# Cell 10: TrainingArguments + Trainer init\n# - Configured for 2xT4 training\n# - Checkpointing to WORKDIR\n# ==========================\n\ntraining_args = TrainingArguments(\n    output_dir=WORKDIR,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    fp16=True,\n    num_train_epochs=3,\n    logging_steps=50,\n    save_steps=500,\n    save_total_limit=5,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    learning_rate=2e-4,\n    remove_unused_columns=False,\n    push_to_hub=False,\n)\n\n# ==========================\n# Create Trainer (minimal args only)\n# ==========================\n\ntrainer = MMTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_proc,\n    eval_dataset=val_proc,\n    data_collator=collator\n)\n\n# Now patch missing attributes manually AFTER construction\ntrainer.label_names = [\"labels\"]       # ensures Trainer looks for \"labels\"\ntrainer.tokenizer = tokenizer          # optional but useful for save/train logs\n\nprint(\"Trainer created successfully.\")\nprint(\"label_names =\", trainer.label_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:46:26.595868Z","iopub.execute_input":"2025-12-12T16:46:26.596524Z","iopub.status.idle":"2025-12-12T16:46:26.912374Z","shell.execute_reply.started":"2025-12-12T16:46:26.596494Z","shell.execute_reply":"2025-12-12T16:46:26.911740Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\nTrainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n","output_type":"stream"},{"name":"stdout","text":"Trainer created successfully.\nlabel_names = ['labels']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ==========================\n# Cell 11: Resume logic and run training\n# - If a checkpoint exists in WORKDIR, resume\n# - Save adapter only frequently\n# ==========================\n\n# find latest checkpoint\nckpts = [d for d in os.listdir(WORKDIR) if d.startswith(\"checkpoint\")]\nckpt_to_resume = None\nif ckpts:\n    ckpts_sorted = sorted(ckpts, key=lambda x: int(x.split('-')[-1]) if '-' in x else 0)\n    ckpt_to_resume = os.path.join(WORKDIR, ckpts_sorted[-1])\n    print(\"Resuming from checkpoint:\", ckpt_to_resume)\n\ntrainer.train(resume_from_checkpoint=ckpt_to_resume if ckpt_to_resume else None)\n\n# Save the small adapter weights after training\nADAPTER_DIR = os.path.join(WORKDIR, \"final_adapter\")\nos.makedirs(ADAPTER_DIR, exist_ok=True)\nmodel.save_pretrained(ADAPTER_DIR)\ntokenizer.save_pretrained(ADAPTER_DIR)\nprint(\"Adapter saved to\", ADAPTER_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T16:46:30.889846Z","iopub.execute_input":"2025-12-12T16:46:30.890526Z","iopub.status.idle":"2025-12-12T16:47:24.859522Z","shell.execute_reply.started":"2025-12-12T16:46:30.890499Z","shell.execute_reply":"2025-12-12T16:47:24.858373Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2104650368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resuming from checkpoint:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_to_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_to_resume\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mckpt_to_resume\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Save the small adapter weights after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2457\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         wandb_login._login(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    234\u001b[0m     ) -> Tuple[Optional[str], ApiKeyStatus]:\n\u001b[1;32m    235\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             directive = (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local, referrer)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url, referrer)\u001b[0m\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# ==========================\n# Cell 12: Sanity-check inference using base+adapter (fast, optional)\n# - reload base in 4-bit + adapter and run few samples\n# ==========================\n\n# reload base in 4-bit and attach adapter for quick inference\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=False,\n)\nmodel_with_adapter = PeftModel.from_pretrained(base, ADAPTER_DIR)\nmodel_with_adapter.eval()\n\n# run a quick generate on first val example\nif val_proc and len(val_proc) > 0:\n    ex = val_proc[0]\n    batch = collator([ex])\n    device = model_with_adapter.device\n    inputs = {k: v.to(device) for k,v in batch.items() if k in ['input_ids','attention_mask','pixel_values']}\n    with torch.no_grad():\n        gen = model_with_adapter.generate(**inputs, max_new_tokens=64)\n        print(\"Generated:\", tokenizer.decode(gen[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T15:17:22.017087Z","iopub.status.idle":"2025-12-12T15:17:22.017311Z","shell.execute_reply.started":"2025-12-12T15:17:22.017206Z","shell.execute_reply":"2025-12-12T15:17:22.017216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Cell 13: Prepare adapter download (copy ADAPTER_DIR to local/Drive/Kaggle dataset)\n# - This cell packages the adapter folder for transfer to P100 session\n# ==========================\n\n# Example: zip the adapter for download\nimport shutil\nshp = os.path.join(WORKDIR, 'adapter_bundle.zip')\nif os.path.exists(shp):\n    os.remove(shp)\nshutil.make_archive(shp.replace('.zip',''), 'zip', ADAPTER_DIR)\nprint('Adapter archive created at', shp)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T15:17:22.018342Z","iopub.status.idle":"2025-12-12T15:17:22.018718Z","shell.execute_reply.started":"2025-12-12T15:17:22.018579Z","shell.execute_reply":"2025-12-12T15:17:22.018593Z"}},"outputs":[],"execution_count":null}]}